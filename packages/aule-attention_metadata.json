{
  "name": "aule-attention",
  "version": "0.4.0",
  "summary": "FlashAttention that just works. No compilation. Any GPU. AMD ROCm, NVIDIA CUDA, Intel, Apple via Vulkan.",
  "author": null,
  "license": "MIT",
  "home_page": null,
  "download_filename": "aule_attention-0.4.0.tar.gz",
  "download_time": "2025-12-17T02:31:57.087624",
  "package_url": "https://pypi.org/project/aule-attention/"
}